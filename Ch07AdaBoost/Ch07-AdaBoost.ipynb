{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost元算法\n",
    " * 前面已经介绍了五种不同的分类算法，它们各有优缺点。我们自然可以将不同的分类器组合起来，而这种组合结果则被称为集成方法（ensemble method）或者元算法（meta-algorithm）。使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。接下来，我们将介绍基于同一种分类器多个不同实例的两种计算方法。在这些方法当中，数据集也会不断变化，而后应用于不同的实例分类器上。最后，我们会讨论如何利用机器学习问题的通用框架来应用AdaBoost算法。\n",
    "   * bagging：基于数据随机重抽样的分类器构建方法\n",
    "   * boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于单层决策树（decision stump，也称决策树桩）构建弱分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSimpData():\n",
    "    datMat = matrix([[1., 2.1],\n",
    "                    [2., 1.1],\n",
    "                    [1.3, 1.],\n",
    "                    [1., 1.],\n",
    "                    [2., 1.]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat, classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "datMat, classLabels = loadSimpData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单层决策树生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于测试是否有某个值小于或者大于我们正在测试的阈值\n",
    "def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n",
    "    retArray = ones((shape(dataMatrix)[0], 1))\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:, dimen] > threshVal] = -1.0\n",
    "    return retArray\n",
    "# 在一个加权数据集中循环，并找到具有最低错误率的单层决策树\n",
    "def buildStump(dataArr, classLabels, D):\n",
    "    dataMatrix = mat(dataArr)\n",
    "    labelMat = mat(classLabels).T\n",
    "    m, n = shape(dataMatrix)\n",
    "    numSteps = 10.0\n",
    "    bestStump = {}\n",
    "    bestClassEst = mat(zeros((m, 1)))\n",
    "    minError = inf\n",
    "    for i in range(n):\n",
    "        rangeMin = dataMatrix[:, i].min()\n",
    "        rangeMax = dataMatrix[:, i].max()\n",
    "        stepSize = (rangeMax-rangeMin)/numSteps\n",
    "        for j in range(-1, int(numSteps) + 1):\n",
    "            for inequal in ['lt', 'gt']:\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                predictedVals = \\\n",
    "                        stumpClassify(dataMatrix, i, threshVal, inequal)\n",
    "                errArr = mat(ones((m,1)))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                # 计算加权错误率\n",
    "                weightedError = D.T*errArr\n",
    "#                 print 'split: dim %d, thresh %.2f, thresh ineqal: \\ \n",
    "#                        %s, the weighted error is %.3f' % (i, threshVal, inequal, weightedError)\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClassEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClassEst\n",
    "\n",
    "# 官方文档\n",
    "# def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):#just classify the data\n",
    "#     retArray = ones((shape(dataMatrix)[0],1))\n",
    "#     if threshIneq == 'lt':\n",
    "#         retArray[dataMatrix[:,dimen] <= threshVal] = -1.0\n",
    "#     else:\n",
    "#         retArray[dataMatrix[:,dimen] > threshVal] = -1.0\n",
    "#     return retArray\n",
    "    \n",
    "\n",
    "# def buildStump(dataArr,classLabels,D):\n",
    "#     dataMatrix = mat(dataArr); labelMat = mat(classLabels).T\n",
    "#     m,n = shape(dataMatrix)\n",
    "#     numSteps = 10.0; bestStump = {}; bestClasEst = mat(zeros((m,1)))\n",
    "#     minError = inf #init error sum, to +infinity\n",
    "#     for i in range(n):#loop over all dimensions\n",
    "#         rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();\n",
    "#         stepSize = (rangeMax-rangeMin)/numSteps\n",
    "#         for j in range(-1,int(numSteps)+1):#loop over all range in current dimension\n",
    "#             for inequal in ['lt', 'gt']: #go over less than and greater than\n",
    "#                 threshVal = (rangeMin + float(j) * stepSize)\n",
    "#                 predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)#call stump classify with i, j, lessThan\n",
    "#                 errArr = mat(ones((m,1)))\n",
    "#                 errArr[predictedVals == labelMat] = 0\n",
    "#                 weightedError = D.T*errArr  #calc total error multiplied by D\n",
    "#                 #print \"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError)\n",
    "#                 if weightedError < minError:\n",
    "#                     minError = weightedError\n",
    "#                     bestClasEst = predictedVals.copy()\n",
    "#                     bestStump['dim'] = i\n",
    "#                     bestStump['thresh'] = threshVal\n",
    "#                     bestStump['ineq'] = inequal\n",
    "#     return bestStump,minError,bestClasEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'ineq': 'lt', 'thresh': 1.3}, matrix([[0.2]]), array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = mat(ones((5, 1))/5)\n",
    "buildStump(dataMat, classLabels, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整 AdaBoost 算法的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于单层决策树的AdaBoost训练过程\n",
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    D = mat(ones((m, 1))/m)\n",
    "    aggClassEst = mat(zeros((m, 1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n",
    "        print 'D:', D.T\n",
    "        alpha = float(0.5*log((1.0-error)/max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "        print 'classEst: ',classEst.T\n",
    "        # 为下一次迭代计算D\n",
    "        expon = multiply(-1*alpha*mat(classLabels).T, classEst)\n",
    "        D = multiply(D, exp(expon))\n",
    "        D = D/D.sum()\n",
    "        # 错误率累加计算\n",
    "        aggClassEst += alpha*classEst\n",
    "        print 'aggClassEst: ',aggClassEst.T\n",
    "        aggErrors = multiply(sign(aggClassEst) != \n",
    "                            mat(classLabels).T, ones((m, 1)))\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print 'total error:',errorRate, '\\n'\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    return weakClassArr\n",
    "\n",
    "###官方的文档\n",
    "# def adaBoostTrainDS(dataArr,classLabels,numIt=40):\n",
    "#     weakClassArr = []\n",
    "#     m = shape(dataArr)[0]\n",
    "#     D = mat(ones((m,1))/m)   #init D to all equal\n",
    "#     aggClassEst = mat(zeros((m,1)))\n",
    "#     for i in range(numIt):\n",
    "#         bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump\n",
    "#         print \"D:\",D.T\n",
    "#         alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0\n",
    "#         bestStump['alpha'] = alpha  \n",
    "#         weakClassArr.append(bestStump)                  #store Stump Params in Array\n",
    "#         print \"classEst: \",classEst.T\n",
    "#         expon = multiply(-1*alpha*mat(classLabels).T,classEst) #exponent for D calc, getting messy\n",
    "#         D = multiply(D,exp(expon))                              #Calc New D for next iteration\n",
    "#         D = D/D.sum()\n",
    "#         #calc training error of all classifiers, if this is 0 quit for loop early (use break)\n",
    "#         aggClassEst += alpha*classEst\n",
    "#         print \"aggClassEst: \",aggClassEst.T\n",
    "#         aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))\n",
    "#         errorRate = aggErrors.sum()/m\n",
    "#         print \"total error: \",errorRate\n",
    "#         if errorRate == 0.0: break\n",
    "#     return weakClassArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[0.2 0.2 0.2 0.2 0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error: 0.2 \n",
      "\n",
      "D: [[0.5   0.125 0.125 0.125 0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error: 0.2 \n",
      "\n",
      "D: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\n",
      "classEst:  [[1. 1. 1. 1. 1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error: 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifierArray = adaBoostTrainDS(datMat, classLabels, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'alpha': 0.6931471805599453, 'dim': 0, 'ineq': 'lt', 'thresh': 1.3},\n",
       " {'alpha': 0.9729550745276565, 'dim': 1, 'ineq': 'lt', 'thresh': 1.0},\n",
       " {'alpha': 0.8958797346140273, 'dim': 0, 'ineq': 'lt', 'thresh': 0.9}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试算法：基于 AdaBoost 的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
