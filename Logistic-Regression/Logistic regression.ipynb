{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法：Logistic 回归梯度上升优化算法，找到最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lodaDataSet():\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open('testSet.txt')\n",
    "    for line in fr.readlnes():\n",
    "        lineArr = line.strip().split()\n",
    "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
    "        # labelMat 里面是数据的原始类别标签\n",
    "        labelMat.append(int(lineArr[2]))\n",
    "    return dataMat,labelMat\n",
    "\n",
    "def sigmoid(inX):\n",
    "    return 1.0/(1+exp(-inX))\n",
    "\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    # 将数组转化为numpy矩阵\n",
    "    dataMatrix = mat(dataMatIn)\n",
    "    labelMat = mat(classLabels).transpose()\n",
    "    m,n = shape(dataMatrix)\n",
    "    # alpha 是向目标移动的步长\n",
    "    alpha = 0.001\n",
    "    # 迭代次数\n",
    "    maxCycles = 500\n",
    "    weights = ones((n, 1))\n",
    "    for k in range(maxCycles):\n",
    "        h = sigmoid(dataMatrix*weights)\n",
    "        error = (labelMat - h)\n",
    "        # w:=w + alpha × gradient更新回归系数的向量 梯度上升是‘+’，梯度下降是‘-’\n",
    "        # dataMatrix.transpose()*error 的数学推导还不清楚\n",
    "        weights = weights + alpha * dataMatrix.transpose()*error\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析数据：画出决策边界，即画出数据集和Logistic回归最佳拟合直线的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBestFit(wei):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # getA()函数没见过，查清楚\n",
    "    weights = wei.getA()\n",
    "    dataMat, labelMat = loadDataSet()\n",
    "    dataArr = array(dataMat)\n",
    "    n = shape(dataArr)[0]\n",
    "    xcord1 = []; ycord1 = []\n",
    "    xcord2 = []; ycord2 = []\n",
    "    # 将数据点分类\n",
    "    for i in range(n):\n",
    "        if int(labelMat[i]) == 1:\n",
    "            xcord1.append(dataArr[i, 1]); ycord1.append(dataArr[i, 2])\n",
    "        else:\n",
    "            xcord2.append(dataArr[i, 1]); ycord2.append(dataArr[i, 2])\n",
    "    # plt.figure()大概清楚意思，不是很确定，查\n",
    "    fig = plt.figure()\n",
    "    # fig.add_subblot 不熟悉\n",
    "    ax = fig.add_subblot(111)\n",
    "    # scatter 完全不熟\n",
    "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='a')\n",
    "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
    "    x = arange(-3.0, 3.0, 0.1)\n",
    "    # 最佳拟合直线，弄懂\n",
    "    y = (-weights[0] - weights[1]*x)/weights[2]\n",
    "    ax.plot(x, y)\n",
    "    plt.xlabel('X1'); plt.ycord1('X2');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法：随机梯度上升\n",
    " * 一次仅用一个样本点来更新回归系数，该方法称为__随机梯度上升算法__。\n",
    " * 由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个__在线学习算法__。\n",
    " * 与“在线学习”相对应，一次处理所有数据被称作是“批处理”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机梯度上升算法\n",
    "def stocGradAscent0(dataMatrix, classLabels):\n",
    "    m, n = shape(dataMatrix)\n",
    "    # alpha 的值是固定的\n",
    "    alpha = 0.01\n",
    "    # 所有回归系数初始化为1，即迭代从1开始\n",
    "    weights = ones(n)\n",
    "    # 迭代次数由原来的500次变为数据的行数m\n",
    "    for i in range(m):\n",
    "        # 这里的 h 与之前先比已经变成了数值，而不再是向量\n",
    "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
    "        # error 也是数值了\n",
    "        error = classLabels[i] - h \n",
    "        weights = weights + alpha * error * dataMatrix[i]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进的随机梯度上升算法\n",
    " * 对于一些特殊场景的问题的解决和改进\n",
    " * ？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这种改进的目的是什么没看懂\n",
    "# 算法增加了一个迭代次数作为第3个参数。如果该参数没有给定的话，算法将默认迭代150次。\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    m, n = shape(dataMatrix)\n",
    "    weights = ones(n)\n",
    "    for j in range(numIter):\n",
    "        dataIndex = range(m)\n",
    "        for i in range(m):\n",
    "            alpha = 4/(1.0+j+i)+0.0001\n",
    "            randIndex = int(random.uniform(0, len(dataIndex)))\n",
    "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            del(dataIndex[randIndex])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  示例：从疝气病症预测病马的死亡率\n",
    " * (1) 收集数据：给定数据文件。\n",
    " * (2) 准备数据：__用Python解析文本文件并填充缺失值。__\n",
    " * (3) 分析数据：__可视化并观察数据。__\n",
    " * (4) 训练算法：使用优化算法，找到最佳的系数。\n",
    " * (5) 测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数。\n",
    " * (6) 使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以做为留给读者的一道习题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*该数据还存在一个问题，__数据集中有30%的值是缺失的。下面将首先介绍如何处理数据集中的数据缺失问题__，然后再利用Logistic回归和随机梯度上升算法来预测病马的生死。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据：处理有缺失值的数据的一些可先办法\n",
    " * 使用可用特征的均值来填补缺失值；\n",
    " * 使用特殊值来填补缺失值，如-1；\n",
    " * 忽略有缺失值的样本；\n",
    " * 使用相似样本的均值添补缺失值；\n",
    " * 使用另外的机器学习算法预测缺失值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对书中的数据，作者做出了如下处理，并给出了解释；\n",
    " * 选择实数0来替换所有缺失值\n",
    "   * 首先NumPy数据类型不允许包含缺失值；\n",
    "   * 在更新时不会影响系数的值；\n",
    "   * 由于sigmoid(0)=0.5，即它对结果的预测不具有任何倾向性，因此上述做法也不会对误差项造成任何影响。\n",
    " * 如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。\n",
    "   * 因为类别标签与特征不同，很难确定采用某个合适的值来替换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试算法：用 Logistic 回归进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic回归分类函数\n",
    "def classidfyVector(inX, weights):\n",
    "    prob = sigmoid(sum(inX*weights))\n",
    "    if prob > 0.5: return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def colicTest():\n",
    "    frTrain = open('horseColicTraining.txt')\n",
    "    frTest = open('horseColicTest.txt')\n",
    "    trainingSet = []; trainingLabels = []\n",
    "    for line in frTrain.readlines():\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        trainingSet.append(lineArr)\n",
    "        trainingLabels.append(float(currLine[21]))\n",
    "    traiWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500)\n",
    "    errorCount = 0; numTestVec = 0.0\n",
    "    for line in frTest.readlines():\n",
    "        numTestVec += 1.0\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        if int(classidfyVector(array(lineArr), traiWeights)) != int(currLine[21]):\n",
    "            errorCount += 1\n",
    "    errorRate = (float(errorCount)/numTestVec)\n",
    "    print 'the error rate of this test is: %f' % errorRate\n",
    "    return errorRate\n",
    "\n",
    "def multiTest():\n",
    "    numTests = 10; errorSum = 0.0\n",
    "    for k in range(numTests):\n",
    "        errorSum += colicTest()\n",
    "    print \"after %d iterations the average error rate is: \n",
    "        %f\" % (numTests, errorSum/float(numTests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
