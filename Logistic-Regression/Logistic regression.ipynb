{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法：Logistic 回归梯度上升优化算法，找到最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lodaDataSet():\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open('testSet.txt')\n",
    "    for line in fr.readlnes():\n",
    "        lineArr = line.strip().split()\n",
    "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
    "        labelMat.append(int(lineArr[2]))\n",
    "    return dataMat,labelMat\n",
    "\n",
    "def sigmoid(inX):\n",
    "    return 1.0/(1+exp(-inX))\n",
    "\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    # 将数组转化为numpy矩阵\n",
    "    dataMatrix = mat(dataMatIn)\n",
    "    labelMat = mat(classLabels).transpose()\n",
    "    m,n = shape(dataMatrix)\n",
    "    # alpha 是向目标移动的步长\n",
    "    alpha = 0.001\n",
    "    # 迭代次数\n",
    "    maxCycles = 500\n",
    "    weights = ones((n, 1))\n",
    "    for k in range(maxCycles):\n",
    "        h = sigmoid(dataMatrix*weights)\n",
    "        error = (labelMat - h)\n",
    "        # w:=w + alpha × gradient更新回归系数的向量 梯度上升是‘+’，梯度下降是‘-’\n",
    "        # dataMatrix.transpose()*error 的数学推导还不清楚\n",
    "        weights = weights + alpha * dataMatrix.transpose()*error\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析数据：画出决策边界，即画出数据集和Logistic回归最佳拟合直线的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBestFit(wei):\n",
    "    import matplotlib.pyplot as plt\n",
    "    weights = wei.getA()\n",
    "    dataMat, labelMat = loadDataSet()\n",
    "    dataArr = array(dataMat)\n",
    "    n = shape(dataArr)[0]\n",
    "    xcord1 = []; ycord1 = []\n",
    "    xcord2 = []; ycord2 = []\n",
    "    for i in range(n):\n",
    "        if int(labelMat[i]) == 1:\n",
    "            xcord1.append(dataArr[i, 1]); ycord1.append(dataArr[i, 2])\n",
    "        else:\n",
    "            xcord2.append(dataArr[i, 1]); ycord2.append(dataArr[i, 2])\n",
    "    fig = plt.figure():\n",
    "    ax = fig.add_subblot(111)\n",
    "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='a')\n",
    "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
    "    x = arange(-3.0, 3.0, 0.1)\n",
    "    y = (-weights[0] - weights[1]*x)/weights[2]\n",
    "    ax.plot(x, y)\n",
    "    plt.xlabel('X1'); plt.ycord1('X2');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法：随机梯度上升\n",
    " * 一次仅用一个样本点来更新回归系数，该方法称为__随机梯度上升算法__。\n",
    " * 由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个__在线学习算法__。\n",
    " * 与“在线学习”相对应，一次处理所有数据被称作是“批处理”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机梯度上升算法\n",
    "def stocGradAscent0(dataMatrix, classLabels):\n",
    "    m, n = shape(dataMatrix)\n",
    "    alpha = 0.01\n",
    "    weights = ones(n)\n",
    "    for i in range(m):\n",
    "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
    "        error = classLabels[i] - h \n",
    "        weights = weights + alpha * dataMatrix[i]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进的随机梯度上升算法\n",
    " * 对于一些特殊场景的问题的解决和改进\n",
    " * ？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    m, n = shape(dataMatrix)\n",
    "    weights = ones(n)\n",
    "    for j in range(numIter):\n",
    "        dataIndex = range(m)\n",
    "        for i in range(m):\n",
    "            alpha = 4/(1.0+j+i)+0.0001\n",
    "            randIndex = int(random.uniform(0, len(dataIndex)))\n",
    "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            del(dataIndex[randIndex])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  示例：从疝气病症预测病马的死亡率\n",
    " * (1) 收集数据：给定数据文件。\n",
    " * (2) 准备数据：__用Python解析文本文件并填充缺失值。__\n",
    " * (3) 分析数据：__可视化并观察数据。__\n",
    " * (4) 训练算法：使用优化算法，找到最佳的系数。\n",
    " * (5) 测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数。\n",
    " * (6) 使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以做为留给读者的一道习题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*该数据还存在一个问题，__数据集中有30%的值是缺失的。下面将首先介绍如何处理数据集中的数据缺失问题__，然后再利用Logistic回归和随机梯度上升算法来预测病马的生死。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据：处理数据中的缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic回归分类函数\n",
    "def classidfyVector(inX, weights):\n",
    "    prob = sigmoid(sum(inX*weights))\n",
    "    if prob > 0.5: return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def colicTest():\n",
    "    frTrain = open('horseColicTraining.txt')\n",
    "    frTest = open('horseColicTest.txt')\n",
    "    trainingSet = []; trainingLabels = []\n",
    "    for line in frTrain.readlines():\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        trainingSet.append(lineArr)\n",
    "        trainingLabels.append(float(currLine[21]))\n",
    "    traiWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500)\n",
    "    errorCount = 0; numTestVec = 0.0\n",
    "    for line in frTest.readlines():\n",
    "        numTestVec += 1.0\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        if int(classidfyVector(array(lineArr), traiWeights)) != int(currLine[21]):\n",
    "            errorCount += 1\n",
    "    errorRate = (float(errorCount)/numTestVec)\n",
    "    print 'the error rate of this test is: %f' % errorRate\n",
    "    return errorRate\n",
    "\n",
    "def multiTest():\n",
    "    numTests = 10; errorSum = 0.0\n",
    "    for k in range(numTests):\n",
    "        errorSum += colicTest()\n",
    "    print \"after %d iterations the average error rate is: \n",
    "        %f\" % (numTests, errorSum/float(numTests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
